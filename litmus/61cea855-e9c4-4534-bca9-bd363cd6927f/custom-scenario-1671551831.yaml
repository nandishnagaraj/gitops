apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  creationTimestamp: null
  labels:
    cluster_id: 61dd221f-824e-4f36-acff-a28fa3f55724
    subject: custom-scenario-1671551831_litmus
    workflow_id: 29e7e0c1-8f2f-48fa-88da-46b11df49265
    workflows.argoproj.io/controller-instanceid: 61dd221f-824e-4f36-acff-a28fa3f55724
  name: custom-scenario-1671551831
  namespace: litmus
spec:
  arguments:
    parameters:
    - name: TARGET_NODE
      value: gke-hcllink-default-pool-7d131db4-qn6n
    - name: APP_NAMESPACE
      value: k1bigfix
    - name: APP_LABEL
      value: app.kubernetes.io/instance=k1bigfix
    - name: adminModeNamespace
      value: litmus
  entrypoint: custom-chaos
  nodeSelector:
    chaos: "true"
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
  serviceAccountName: argo-chaos
  templates:
  - inputs: {}
    metadata: {}
    name: custom-chaos
    outputs: {}
    steps:
    - - arguments: {}
        name: install-chaos-experiments
        template: install-chaos-experiments
    - - arguments: {}
        name: start-ssh-bridge
        template: start-ssh-bridge
    - - arguments:
          parameters:
          - name: ssh-internal-ip
            value: '{{steps.start-ssh-bridge.outputs.parameters.ssh-bridge-pod-ip}}'
        name: node-restart
        template: node-restart
      - arguments: {}
        name: node-restart-placeholder
        template: node-restart-placeholder
  - inputs:
      artifacts:
      - name: node-restart
        path: /tmp/node-restart.yaml
        raw:
          data: |
            apiVersion: litmuschaos.io/v1alpha1
            description:
              message: |
                Restart node
            kind: ChaosExperiment
            metadata:
              name: node-restart
              labels:
                name: node-restart
                app.kubernetes.io/part-of: litmus
                app.kubernetes.io/component: chaosexperiment
                app.kubernetes.io/version: 2.9.0
            spec:
              definition:
                scope: Cluster
                permissions:
                  # Create and monitor the experiment & helper pods
                  - apiGroups: [""]
                    resources: ["pods"]
                    verbs: ["create","delete","get","list","patch","update", "deletecollection"]
                  # Performs CRUD operations on the events inside chaosengine and chaosresult
                  - apiGroups: [""]
                    resources: ["events"]
                    verbs: ["create","get","list","patch","update"]
                  # Fetch configmaps & secrets details and mount it to the experiment pod (if specified)
                  - apiGroups: [""]
                    resources: ["configmaps","secrets"]
                    verbs: ["get","list",]
                  # Track and get the runner, experiment, and helper pods log
                  - apiGroups: [""]
                    resources: ["pods/log"]
                    verbs: ["get","list","watch"]
                  # for creating and managing to execute comands inside target container
                  - apiGroups: [""]
                    resources: ["pods/exec"]
                    verbs: ["get","list","create"]
                  # for configuring and monitor the experiment job by the chaos-runner pod
                  - apiGroups: ["batch"]
                    resources: ["jobs"]
                    verbs: ["create","list","get","delete","deletecollection"]
                  # for creation, status polling and deletion of litmus chaos resources used within a chaos workflow
                  - apiGroups: ["litmuschaos.io"]
                    resources: ["chaosengines","chaosexperiments","chaosresults"]
                    verbs: ["create","list","get","patch","update","delete"]
                  # for experiment to perform node status checks
                  - apiGroups: [""]
                    resources: ["nodes"]
                    verbs: ["get","list"]
                image: "litmuschaos/go-runner:2.9.0"
                imagePullPolicy: Always
                args:
                - -c
                - ./experiments -name node-restart
                command:
                - /bin/bash
                env:
                - name: SSH_USER
                  value: 'root'

                - name: TOTAL_CHAOS_DURATION
                  value: '60'

                # Period to wait before and after injection of chaos in sec
                - name: RAMP_TIME
                  value: ''

                # PROVIDE THE LIB HERE
                # ONLY LITMUS SUPPORTED
                - name: LIB
                  value: 'litmus'

                # provide lib image
                - name: LIB_IMAGE
                  value: "litmuschaos/go-runner:2.9.0"

                # ENTER THE TARGET NODE NAME
                - name: TARGET_NODE
                  value: ''

                - name: NODE_LABEL
                  value: ''

                # ENTER THE TARGET NODE IP
                - name: TARGET_NODE_IP
                  value: ''

                labels:
                  name: node-restart
                  app.kubernetes.io/part-of: litmus
                  app.kubernetes.io/component: experiment-job
                  app.kubernetes.io/version: 2.9.0
                secrets:
                  - name: id-rsa
                    mountPath: /mnt/
    metadata: {}
    name: install-chaos-experiments
    outputs: {}
    script:
      command:
      - sh
      image: litmuschaos/k8s:latest
      name: ""
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        readOnlyRootFilesystem: true
      source: |
        #!/bin/sh
        set -euxo pipefail
        kubectl apply -f /tmp/node-restart.yaml -n {{workflow.parameters.adminModeNamespace}}
  - inputs:
      artifacts:
      - name: bridge-pod
        path: /tmp/ssh-bridge.yaml
        raw:
          data: |
            apiVersion: v1
            kind: Pod
            metadata:
              name: ssh-bridge
            spec:
              hostPID: true
              nodeSelector:
                kubernetes.io/hostname: "{{workflow.parameters.TARGET_NODE}}"
              restartPolicy: Never
              serviceAccountName: argo-chaos
              containers:
                - name: alpine
                  image: alpine:3.16.0
                  command: ['sh']
                  args: ['-x', '/opt/startup.sh']
                  securityContext:
                    privileged: true
                  ports:
                    - containerPort: 22
                  volumeMounts:
                    - name: ssh-bridge-helpers
                      subPath: startup.sh
                      mountPath: /opt/startup.sh
                    - name: ssh-bridge-helpers
                      subPath: label_placeholder.sh
                      mountPath: /opt/label_placeholder.sh
              volumes:
                - name: ssh-bridge-helpers
                  configMap:
                    name: ssh-bridge-helpers
      - name: helpers-configmap
        path: /tmp/helpers-configmap.yaml
        raw:
          data: |
            apiVersion: v1
            kind: ConfigMap
            metadata:
              name: ssh-bridge-helpers
            data:
              startup.sh: |
                #!/bin/sh
                set -euxo pipefail
                apk add --no-cache openssh-server
                apk add --no-cache --repository http://dl-cdn.alpinelinux.org/alpine/edge/testing kubectl
                sed -i s/root:!/"root:*"/g /etc/shadow
                echo "export KUBERNETES_PORT=${KUBERNETES_PORT}" >> /opt/kubectl-connect.sh
                echo "export KUBERNETES_SERVICE_PORT=${KUBERNETES_SERVICE_PORT}" >> /opt/kubectl-connect.sh
                echo "export KUBERNETES_PORT_443_TCP_ADDR=${KUBERNETES_PORT_443_TCP_ADDR}" >> /opt/kubectl-connect.sh
                echo "export KUBERNETES_PORT_443_TCP_PORT=${KUBERNETES_PORT_443_TCP_PORT}" >> /opt/kubectl-connect.sh
                echo "export KUBERNETES_PORT_443_TCP_PROTO=${KUBERNETES_PORT_443_TCP_PROTO}" >> /opt/kubectl-connect.sh
                echo "export KUBERNETES_PORT_443_TCP=${KUBERNETES_PORT_443_TCP}" >> /opt/kubectl-connect.sh
                echo "export KUBERNETES_SERVICE_PORT_HTTPS=${KUBERNETES_SERVICE_PORT_HTTPS}" >> /opt/kubectl-connect.sh
                echo "export KUBERNETES_SERVICE_HOST=${KUBERNETES_SERVICE_HOST}" >> /opt/kubectl-connect.sh
                ssh-keygen -A
                ssh-keygen -t ed25519 -f /root/.ssh/id_ed25519 -N ""
                kubectl create secret generic -n {{workflow.parameters.adminModeNamespace}} id-rsa \
                  --from-file=ssh-privatekey=/root/.ssh/id_ed25519 \
                  --type=kubernetes.io/ssh-auth
                mv /root/.ssh/id_ed25519.pub /root/.ssh/authorized_keys
                chmod 600 /root/.ssh/authorized_keys
                rm /root/.ssh/id_ed25519
                /usr/sbin/sshd -D
              label_placeholder.sh: |
                #!/bin/sh
                set -euxo pipefail
                until (kubectl get pod -n {{workflow.parameters.APP_NAMESPACE}} -o name | grep pod/node-restart-placeholder); do sleep 1; done
                kubectl label pod -n {{workflow.parameters.APP_NAMESPACE}} node-restart-placeholder {{workflow.parameters.APP_LABEL}}
                until (kubectl get pod -n {{workflow.parameters.APP_NAMESPACE}} -l {{workflow.parameters.APP_LABEL}} -o name | grep pod/node-restart-placeholder); do sleep 1; done
    metadata: {}
    name: start-ssh-bridge
    outputs:
      parameters:
      - name: ssh-bridge-pod-ip
        valueFrom:
          path: /mnt/out/pod-ip
    script:
      command:
      - sh
      image: litmuschaos/k8s:latest
      name: ""
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        readOnlyRootFilesystem: true
      source: |
        #!/bin/sh
        set -euxo pipefail
        kubectl delete --ignore-not-found --wait=false -n {{workflow.parameters.APP_NAMESPACE}} pod node-restart-placeholder
        kubectl delete --ignore-not-found --wait=false -n {{workflow.parameters.adminModeNamespace}} pod ssh-bridge
        kubectl delete --ignore-not-found -n {{workflow.parameters.adminModeNamespace}} configmap ssh-bridge-helpers
        kubectl delete --ignore-not-found -n {{workflow.parameters.adminModeNamespace}} secret id-rsa
        cat /tmp/helpers-configmap.yaml
        kubectl apply -f /tmp/helpers-configmap.yaml -n {{workflow.parameters.adminModeNamespace}}
        cat /tmp/ssh-bridge.yaml
        kubectl apply -f /tmp/ssh-bridge.yaml -n {{workflow.parameters.adminModeNamespace}}
        kubectl wait pods -n {{workflow.parameters.adminModeNamespace}} ssh-bridge --for condition=Ready --timeout=90s
        kubectl get pod -n {{workflow.parameters.adminModeNamespace}} ssh-bridge -o jsonpath='{.status.podIP}' > /mnt/out/pod-ip
      volumeMounts:
      - mountPath: /mnt/out
        name: out
    volumes:
    - emptyDir: {}
      name: out
  - inputs:
      artifacts:
      - name: node-restart-placeholder
        path: /tmp/node-restart-placeholder.yaml
        raw:
          data: |
            apiVersion: v1
            kind: Pod
            metadata:
              name: node-restart-placeholder
            spec:
              containers:
                - name: placeholder
                  image: alpine:3.16.0
                  command: ['sleep']
                  args: ['3600']
                  startupProbe:
                    exec:
                      command: ['false']
                    initialDelaySeconds: 0
                    failureThreshold: 24
                    periodSeconds: 3600
              nodeSelector:
                chaos: "true"
              tolerations:
                - key: "chaos"
                  operator: "Equal"
                  value: "true"
                  effect: "NoSchedule"
    metadata: {}
    name: node-restart-placeholder
    outputs: {}
    script:
      command:
      - sh
      image: litmuschaos/k8s:latest
      name: ""
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        readOnlyRootFilesystem: true
      source: |
        #!/bin/sh
        set -euxo pipefail
        kubectl apply -f /tmp/node-restart-placeholder.yaml -n {{workflow.parameters.APP_NAMESPACE}}
        until (kubectl get pod -n {{workflow.parameters.APP_NAMESPACE}} -l {{workflow.parameters.APP_LABEL}} -o name | grep pod/node-restart-placeholder); do sleep 1; done
        kubectl delete --ignore-not-found -n {{workflow.parameters.adminModeNamespace}} secret id-rsa
        while kubectl wait pods -n {{workflow.parameters.APP_NAMESPACE}} -l {{workflow.parameters.APP_LABEL}} --field-selector spec.nodeName={{workflow.parameters.TARGET_NODE}} --for condition=Ready --timeout=30s; do sleep 5; done
        kubectl delete --ignore-not-found --wait=false -n {{workflow.parameters.APP_NAMESPACE}} pod node-restart-placeholder
        kubectl delete --ignore-not-found --wait=false -n {{workflow.parameters.adminModeNamespace}} pod ssh-bridge
        kubectl delete --ignore-not-found -n {{workflow.parameters.adminModeNamespace}} configmap ssh-bridge-helpers
  - container:
      args:
      - -file=/tmp/chaosengine-node-restart.yaml
      - -saveName=/tmp/engine-name
      image: litmuschaos/litmus-checker:latest
      name: ""
      resources: {}
    inputs:
      artifacts:
      - name: node-restart
        path: /tmp/chaosengine-node-restart.yaml
        raw:
          data: |
            apiVersion: litmuschaos.io/v1alpha1
            kind: ChaosEngine
            metadata:
              namespace: "{{workflow.parameters.adminModeNamespace}}"
              generateName: node-restart
              labels:
                instance_id: 16ece9d3-7f3e-4d26-af85-5a720ffb62be
                context: node-restart_litmus
            spec:
              jobCleanUpPolicy: retain
              engineState: active
              annotationCheck: "false"
              auxiliaryAppInfo: ""
              chaosServiceAccount: litmus-admin
              components:
                runner:
                  nodeSelector: # nodeSelector controls placement of chaos runner (spawned by chaos operator)
                    chaos: "true"
                  tolerations: # tolerations apply to chaos runner (spawned by chaos operator)
                    - key: "chaos"
                      operator: "Equal"
                      value: "true"
                      effect: "NoSchedule"
              experiments:
                - name: node-restart
                  spec:
                    components:
                      nodeSelector: # nodeSelector controls placement of node drain experiment pod (spawned by chaos runner)
                        chaos: "true"
                      tolerations: # tolerations apply to node drain experiment pod (spawned by chaos runner)
                        - key: "chaos"
                          operator: "Equal"
                          value: "true"
                          effect: "NoSchedule"
                      env:
                        - name: TARGET_NODE
                          value: "{{workflow.parameters.TARGET_NODE}}"
                        - name: TARGET_NODE_IP
                          value: "{{inputs.parameters.ssh-internal-ip}}"
                        - name: APP_NAMESPACE
                          value: "{{workflow.parameters.APP_NAMESPACE}}"
                        - name: APP_LABEL
                          value: "{{workflow.parameters.APP_LABEL}}"
                        - name: REBOOT_COMMAND
                          value: '"source /opt/kubectl-connect.sh && sh /opt/label_placeholder.sh && nsenter -t 1 -m -u -i -n reboot 5"'
                      statusCheckTimeouts:
                        delay: 2 # delay controls how often to check for AUT containers readiness
                        timeout: 2700 # delay controls timeout (how long to allow waiting) for AUT containers readiness
      parameters:
      - name: ssh-internal-ip
    metadata:
      labels:
        weight: "10"
    name: node-restart
    outputs: {}
  tolerations:
  - effect: NoSchedule
    key: chaos
    operator: Equal
    value: "true"
status:
  finishedAt: null
  startedAt: null
