apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  creationTimestamp: null
  labels:
    cluster_id: 61dd221f-824e-4f36-acff-a28fa3f55724
    subject: custom-scenario-1673254684_litmus
    workflow_id: 8f780717-1f08-4024-a9e0-c5b1fbf6d449
    workflows.argoproj.io/controller-instanceid: 61dd221f-824e-4f36-acff-a28fa3f55724
  name: custom-scenario-1673254684
  namespace: litmus
spec:
  arguments:
    parameters:
    - name: TARGET_NODE
      value: gke-hcllink-default-pool-7d131db4-fdth
    - name: DESTINATION_NODE
      value: gke-hcllink-default-pool-7d131db4-fotf
    - name: APP_NAMESPACE
      value: k1bigfix
    - name: APP_LABEL
      value: app.kubernetes.io/instance=k1bigfix
    - name: TOTAL_CHAOS_DURATION
      value: "500"
    - name: adminModeNamespace
      value: litmus
    - name: EXTRA_UNCORDON_DELAY
      value: "60"
    - name: DOMAIN
      value: 34.150.158.161
  entrypoint: custom-chaos
  nodeSelector:
    chaos: "true"
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
  serviceAccountName: argo-chaos
  templates:
  - inputs: {}
    metadata: {}
    name: custom-chaos
    outputs: {}
    steps:
    - - arguments: {}
        name: install-chaos-experiments
        template: install-chaos-experiments
    - - arguments: {}
        name: cordon-destination-node
        template: cordon-destination-node
    - - arguments: {}
        name: node-drain
        template: node-drain
      - arguments: {}
        name: uncordon-destination-node
        template: uncordon-destination-node
  - inputs: {}
    metadata: {}
    name: cordon-destination-node
    outputs: {}
    podSpecPatch: '{"serviceAccountName": "node-chaos"}'
    script:
      command:
      - sh
      env:
      - name: DESTINATION_NODE
        value: '{{workflow.parameters.DESTINATION_NODE}}'
      image: litmuschaos/k8s:latest
      name: ""
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        readOnlyRootFilesystem: true
      source: |
        #!/bin/sh
        set -euxo pipefail
        kubectl cordon $DESTINATION_NODE
  - inputs:
      artifacts:
      - name: node-drain
        path: /tmp/node-drain.yaml
        raw:
          data: |
            apiVersion: litmuschaos.io/v1alpha1
            description:
              message: |
                Drain the node where application pod is scheduled
            kind: ChaosExperiment
            metadata:
              name: node-drain
              labels:
                name: node-drain
                app.kubernetes.io/part-of: litmus
                app.kubernetes.io/component: chaosexperiment
                app.kubernetes.io/version: 2.7.0
            spec:
              definition:
                scope: Cluster
                permissions:
                  - apiGroups:
                      - ""
                      - batch
                      - litmuschaos.io
                      - apps
                    resources:
                      - jobs
                      - pods
                      - events
                      - pods/log
                      - pods/exec
                      - daemonsets
                      - pods/eviction
                      - chaosengines
                      - chaosexperiments
                      - chaosresults
                    verbs:
                      - create
                      - list
                      - get
                      - patch
                      - update
                      - delete
                      - deletecollection
                  - apiGroups:
                      - ""
                    resources:
                      - nodes
                    verbs:
                      - get
                      - list
                      - patch
                image: litmuschaos/go-runner:2.7.0
                imagePullPolicy: Always
                args:
                  - -c
                  - ./experiments -name node-drain
                command:
                  - /bin/bash
                env:
                  - name: TARGET_NODE
                    value: ""
                  - name: NODE_LABEL
                    value: ""
                  - name: TOTAL_CHAOS_DURATION
                    value: "600"
                  - name: LIB
                    value: litmus
                  - name: RAMP_TIME
                    value: ""
                labels:
                  name: node-drain
                  app.kubernetes.io/part-of: litmus
                  app.kubernetes.io/component: experiment-job
                  app.kubernetes.io/version: 2.7.0
    metadata: {}
    name: install-chaos-experiments
    outputs: {}
    script:
      command:
      - sh
      image: litmuschaos/k8s:latest
      name: ""
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        readOnlyRootFilesystem: true
      source: |
        #!/bin/sh
        set -euxo pipefail
        kubectl apply -f /tmp/node-drain.yaml -n {{workflow.parameters.adminModeNamespace}} | sleep 5
  - inputs: {}
    metadata: {}
    name: uncordon-destination-node
    outputs: {}
    podSpecPatch: '{"serviceAccountName": "node-chaos"}'
    script:
      command:
      - sh
      env:
      - name: TARGET_NODE
        value: '{{workflow.parameters.TARGET_NODE}}'
      - name: DESTINATION_NODE
        value: '{{workflow.parameters.DESTINATION_NODE}}'
      - name: APP_NAMESPACE
        value: '{{workflow.parameters.APP_NAMESPACE}}'
      - name: EXTRA_UNCORDON_DELAY
        value: '{{workflow.parameters.EXTRA_UNCORDON_DELAY}}'
      image: litmuschaos/k8s:latest
      name: ""
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        readOnlyRootFilesystem: true
      source: |
        #!/bin/sh
        set -euo pipefail
        # remaining="$(kubectl get pods --field-selector spec.nodeName=$TARGET_NODE --namespace $APP_NAMESPACE --output custom-columns=':metadata.name' 2>/dev/null | wc -w | xargs echo -n)"
        # until [ "$remaining" -eq 0 ]; do
        #   echo "Waiting for $remaining application pods to be evicted from the target node"
        #   sleep 5
        #   remaining="$(kubectl get pods --field-selector spec.nodeName=$TARGET_NODE --namespace $APP_NAMESPACE --output custom-columns=':metadata.name' 2>/dev/null | wc -w | xargs echo -n)"
        # done
        # echo "All application pods evicted from target node, waiting an additional ${EXTRA_UNCORDON_DELAY}s"
        sleep $EXTRA_UNCORDON_DELAY
        echo "Uncordoning node $DESTINATION_NODE"
        kubectl uncordon $DESTINATION_NODE
  - container:
      args:
      - -file=/tmp/chaosengine-node-drain.yaml
      - -saveName=/tmp/engine-name
      image: litmuschaos/litmus-checker:latest
      name: ""
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        readOnlyRootFilesystem: true
    inputs:
      artifacts:
      - name: node-drain
        path: /tmp/chaosengine-node-drain.yaml
        raw:
          data: |
            apiVersion: litmuschaos.io/v1alpha1
            kind: ChaosEngine
            metadata:
              namespace: "{{workflow.parameters.adminModeNamespace}}"
              generateName: node-drain
            spec:
              jobCleanUpPolicy: retain
              engineState: active
              auxiliaryAppInfo: ""
              chaosServiceAccount: litmus-admin
              components:
                runner:
                  nodeSelector: # nodeSelector controls placement of chaos runner (spawned by chaos operator)
                    chaos: "true"
                  tolerations: # tolerations apply to chaos runner (spawned by chaos operator)
                    - key: "chaos"
                      operator: "Equal"
                      value: "true"
                      effect: "NoSchedule"
              experiments:
                - name: node-drain
                  spec:
                    components:
                      nodeSelector: # nodeSelector controls placement of node drain experiment pod (spawned by chaos runner)
                        chaos: "true"
                      tolerations: # tolerations apply to node drain experiment pod (spawned by chaos runner)
                        - key: "chaos"
                          operator: "Equal"
                          value: "true"
                          effect: "NoSchedule"
                      env:
                        - name: TARGET_NODE
                          value: "{{workflow.parameters.TARGET_NODE}}"
                        - name: APP_NAMESPACE
                          value: "{{workflow.parameters.APP_NAMESPACE}}"
                        - name: APP_LABEL
                          value: "{{workflow.parameters.APP_LABEL}}"
                        - name: TOTAL_CHAOS_DURATION
                          value: "{{workflow.parameters.TOTAL_CHAOS_DURATION}}"
                      statusCheckTimeouts:
                        delay: 2 # delay controls how often to check for AUT containers readiness
                        timeout: 2700 # timeout controls how long to allow waiting for AUT containers readiness
                    probe:
                      - name: Bigfix-availability-start-probe
                        type: httpProbe
                        mode: SOT
                        runProperties:
                          probeTimeout: 20
                          retry: 3
                          interval: 20
                          probePollingInterval: 1
                        httpProbe/inputs:
                          url: 'https://bigfix-webreports-0.{{workflow.parameters.DOMAIN}}.nip.io/login'
                          insecureSkipVerify: true
                          responseTimeout: 5000
                          method:
                            get:
                              criteria: '=='
                              responseCode: '200'
                      - name: Bigfix-availability-continuous-probe
                        type: httpProbe
                        mode: Continuous
                        runProperties:
                          probeTimeout: 30
                          retry: 9
                          interval: 30
                          probePollingInterval: 1
                        httpProbe/inputs:
                          url: 'https://bigfix-webreports-0.{{workflow.parameters.DOMAIN}}.nip.io/login'
                          insecureSkipVerify: true
                          responseTimeout: 5000
                          method:
                            get:
                              criteria: '=='
                              responseCode: '200'
                      - name: Bigfix-availability-end-probe
                        type: httpProbe
                        mode: EOT
                        runProperties:
                          probeTimeout: 20
                          retry: 3
                          interval: 20
                          probePollingInterval: 1
                        httpProbe/inputs:
                          url: 'https://bigfix-webreports-0.{{workflow.parameters.DOMAIN}}.nip.io/login'
                          insecureSkipVerify: true
                          responseTimeout: 5000
                          method:
                            get:
                              criteria: '=='
                              responseCode: '200'
              annotationCheck: "false"
    metadata:
      labels:
        weight: "10"
    name: node-drain
    outputs: {}
  tolerations:
  - effect: NoSchedule
    key: chaos
    operator: Equal
    value: "true"
status:
  finishedAt: null
  startedAt: null
